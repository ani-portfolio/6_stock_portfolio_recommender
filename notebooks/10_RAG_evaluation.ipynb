{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b9bc75",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77980710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from groq import Groq\n",
    "import re\n",
    "from datetime import datetime\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ded918",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca812d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "GROQ_API_KEY = open(\"/Users/ani/Documents/0_API_KEYS/groq.txt\").read().strip()\n",
    "GROQ_LLM_MODEL = \"llama-3.3-70b-versatile\"\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stock_data(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess stock data from CSV.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Clean column names\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ee324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_groq_response(prompt: str, max_tokens: int = 1000) -> str:\n",
    "    \"\"\"Generate response using Groq API.\"\"\"\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=GROQ_LLM_MODEL,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3303a011",
   "metadata": {},
   "source": [
    "##### Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a6c157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sector_queries(stock_df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"Generate sector-based queries for testing.\"\"\"\n",
    "    sectors = stock_df['Sector'].dropna().unique()\n",
    "    \n",
    "    query_templates = [\n",
    "        \"What are the best {sector} stocks for a conservative portfolio?\",\n",
    "        \"Show me high-growth {sector} companies with good fundamentals\",\n",
    "        \"Which {sector} stocks have the highest dividend yields?\",\n",
    "        \"Find undervalued {sector} stocks with strong performance\",\n",
    "        \"What are the top 5 {sector} stocks by market cap?\"\n",
    "    ]\n",
    "    \n",
    "    test_cases = []\n",
    "    for sector in sectors:\n",
    "        for template in query_templates:\n",
    "            query = template.format(sector=sector)\n",
    "            expected_stocks = stock_df[stock_df['Sector'] == sector]['Ticker'].tolist()\n",
    "            \n",
    "            test_cases.append({\n",
    "                'query': query,\n",
    "                'query_type': 'sector_filter',\n",
    "                'expected_sector': sector,\n",
    "                'expected_stocks': expected_stocks,\n",
    "                'evaluation_criteria': ['sector_relevance', 'stock_recommendations', 'financial_reasoning']\n",
    "            })\n",
    "    \n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc652f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_queries(stock_df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"Generate performance-based queries.\"\"\"\n",
    "    test_cases = [\n",
    "        {\n",
    "            'query': \"Find stocks with the highest annualized returns over 20%\",\n",
    "            'query_type': 'performance_filter',\n",
    "            'filter_condition': lambda df: df['Annualized_Return'] > 20,\n",
    "            'evaluation_criteria': ['numerical_accuracy', 'performance_ranking', 'return_analysis']\n",
    "        },\n",
    "        {\n",
    "            'query': \"Show me stocks with low volatility and positive YTD returns\",\n",
    "            'query_type': 'risk_return',\n",
    "            'filter_condition': lambda df: (df['Annualized_Volatility'] < 25) & (df['YTD_Return'] > 0),\n",
    "            'evaluation_criteria': ['risk_assessment', 'return_analysis', 'portfolio_suitability']\n",
    "        },\n",
    "        {\n",
    "            'query': \"What are the best value stocks with low P/E ratios under 15?\",\n",
    "            'query_type': 'valuation',\n",
    "            'filter_condition': lambda df: df['Trailing_PE'] < 15,\n",
    "            'evaluation_criteria': ['valuation_analysis', 'value_reasoning', 'pe_accuracy']\n",
    "        },\n",
    "        {\n",
    "            'query': \"Find high dividend yield stocks above 3% with stable performance\",\n",
    "            'query_type': 'dividend_focus',\n",
    "            'filter_condition': lambda df: df['Dividend_Yield'] > 3,\n",
    "            'evaluation_criteria': ['dividend_analysis', 'stability_assessment', 'income_focus']\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc556a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison_queries(stock_df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"Generate comparative analysis queries.\"\"\"\n",
    "    test_cases = []\n",
    "    \n",
    "    # Random stock comparisons\n",
    "    tickers = stock_df['Ticker'].dropna().unique()\n",
    "    for _ in range(10):\n",
    "        selected_stocks = random.sample(list(tickers), 3)\n",
    "        \n",
    "        test_cases.append({\n",
    "            'query': f\"Compare {', '.join(selected_stocks)} and recommend which is best for growth investing\",\n",
    "            'query_type': 'stock_comparison',\n",
    "            'target_stocks': selected_stocks,\n",
    "            'evaluation_criteria': ['comparative_analysis', 'growth_metrics', 'recommendation_quality']\n",
    "        })\n",
    "    \n",
    "    return test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80eb0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_portfolio_queries() -> List[Dict]:\n",
    "    \"\"\"Generate portfolio construction queries.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'query': \"Build a balanced portfolio for a 35-year-old with moderate risk tolerance\",\n",
    "            'query_type': 'portfolio_construction',\n",
    "            'investor_profile': {'age': 35, 'risk_tolerance': 'moderate'},\n",
    "            'evaluation_criteria': ['asset_allocation', 'risk_management', 'age_appropriate']\n",
    "        },\n",
    "        {\n",
    "            'query': \"Create a retirement portfolio for someone 60 years old focused on income\",\n",
    "            'query_type': 'portfolio_construction',\n",
    "            'investor_profile': {'age': 60, 'focus': 'income'},\n",
    "            'evaluation_criteria': ['income_generation', 'capital_preservation', 'age_appropriate']\n",
    "        },\n",
    "        {\n",
    "            'query': \"Design an aggressive growth portfolio for a young investor\",\n",
    "            'query_type': 'portfolio_construction',\n",
    "            'investor_profile': {'age': 25, 'risk_tolerance': 'aggressive'},\n",
    "            'evaluation_criteria': ['growth_focus', 'risk_taking', 'long_term_strategy']\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27dd18",
   "metadata": {},
   "source": [
    "##### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f0d4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sector_relevance(response: str, expected_sector: str, stock_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Evaluate if response correctly focuses on the specified sector.\"\"\"\n",
    "    \n",
    "    eval_prompt = f\"\"\"\n",
    "    Evaluate if this stock recommendation response correctly focuses on the {expected_sector} sector.\n",
    "    \n",
    "    Response: {response}\n",
    "    Expected Sector: {expected_sector}\n",
    "    \n",
    "    Rate on a scale of 1-5:\n",
    "    1 = No mention of sector or completely wrong sector\n",
    "    2 = Minimal sector relevance\n",
    "    3 = Some sector relevance but mixed with other sectors\n",
    "    4 = Mostly correct sector focus\n",
    "    5 = Perfect sector focus with detailed sector analysis\n",
    "    \n",
    "    Provide your rating and brief reasoning in JSON format:\n",
    "    {{\"score\": <1-5>, \"reasoning\": \"<explanation>\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation = generate_groq_response(eval_prompt, max_tokens=200)\n",
    "    \n",
    "    try:\n",
    "        eval_result = json.loads(evaluation)\n",
    "        return {\n",
    "            'metric': 'sector_relevance',\n",
    "            'score': eval_result.get('score', 0),\n",
    "            'max_score': 5,\n",
    "            'reasoning': eval_result.get('reasoning', 'No reasoning provided')\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'metric': 'sector_relevance',\n",
    "            'score': 0,\n",
    "            'max_score': 5,\n",
    "            'reasoning': 'Failed to parse evaluation response'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1212e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_stock_recommendations(response: str, expected_stocks: List[str]) -> Dict:\n",
    "    \"\"\"Evaluate quality and relevance of stock recommendations.\"\"\"\n",
    "    \n",
    "    # Extract mentioned tickers from response\n",
    "    mentioned_tickers = re.findall(r'\\b[A-Z]{1,5}\\b', response)\n",
    "    mentioned_tickers = [ticker for ticker in mentioned_tickers if len(ticker) <= 5]\n",
    "    \n",
    "    # Calculate overlap with expected stocks\n",
    "    overlap = len(set(mentioned_tickers) & set(expected_stocks))\n",
    "    precision = overlap / len(mentioned_tickers) if mentioned_tickers else 0\n",
    "    recall = overlap / len(expected_stocks) if expected_stocks else 0\n",
    "    \n",
    "    eval_prompt = f\"\"\"\n",
    "    Evaluate the quality of stock recommendations in this response:\n",
    "    \n",
    "    Response: {response}\n",
    "    Mentioned Tickers: {mentioned_tickers}\n",
    "    \n",
    "    Rate the overall quality of recommendations on 1-5:\n",
    "    1 = No specific stock recommendations\n",
    "    2 = Poor recommendations with little justification\n",
    "    3 = Adequate recommendations with basic reasoning\n",
    "    4 = Good recommendations with solid analysis\n",
    "    5 = Excellent recommendations with comprehensive analysis\n",
    "    \n",
    "    Provide rating in JSON: {{\"score\": <1-5>, \"reasoning\": \"<explanation>\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation = generate_groq_response(eval_prompt, max_tokens=200)\n",
    "    \n",
    "    try:\n",
    "        eval_result = json.loads(evaluation)\n",
    "        quality_score = eval_result.get('score', 0)\n",
    "    except:\n",
    "        quality_score = 0\n",
    "    \n",
    "    # Combine quality and relevance scores\n",
    "    relevance_score = (precision + recall) / 2 * 5  # Convert to 1-5 scale\n",
    "    final_score = (quality_score + relevance_score) / 2\n",
    "    \n",
    "    return {\n",
    "        'metric': 'stock_recommendations',\n",
    "        'score': round(final_score, 2),\n",
    "        'max_score': 5,\n",
    "        'reasoning': f\"Quality: {quality_score}/5, Relevance: {relevance_score:.1f}/5, Precision: {precision:.2f}, Recall: {recall:.2f}\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9588f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_financial_reasoning(response: str) -> Dict:\n",
    "    \"\"\"Evaluate the quality of financial analysis and reasoning.\"\"\"\n",
    "    \n",
    "    eval_prompt = f\"\"\"\n",
    "    Evaluate the financial reasoning and analysis quality in this response:\n",
    "    \n",
    "    Response: {response}\n",
    "    \n",
    "    Consider:\n",
    "    - Use of relevant financial metrics (P/E, ROE, debt ratios, etc.)\n",
    "    - Understanding of risk-return tradeoffs\n",
    "    - Market context and economic factors\n",
    "    - Investment strategy coherence\n",
    "    \n",
    "    Rate on 1-5:\n",
    "    1 = No financial reasoning\n",
    "    2 = Basic mentions of financial concepts\n",
    "    3 = Adequate financial analysis\n",
    "    4 = Strong financial reasoning with multiple metrics\n",
    "    5 = Sophisticated financial analysis with deep insights\n",
    "    \n",
    "    Provide rating in JSON: {{\"score\": <1-5>, \"reasoning\": \"<explanation>\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation = generate_groq_response(eval_prompt, max_tokens=300)\n",
    "    \n",
    "    try:\n",
    "        eval_result = json.loads(evaluation)\n",
    "        return {\n",
    "            'metric': 'financial_reasoning',\n",
    "            'score': eval_result.get('score', 0),\n",
    "            'max_score': 5,\n",
    "            'reasoning': eval_result.get('reasoning', 'No reasoning provided')\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'metric': 'financial_reasoning',\n",
    "            'score': 0,\n",
    "            'max_score': 5,\n",
    "            'reasoning': 'Failed to parse evaluation response'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce01c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_numerical_accuracy(response: str, stock_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Evaluate accuracy of numerical data mentioned in response.\"\"\"\n",
    "    \n",
    "    # Extract potential numerical values and tickers\n",
    "    numbers = re.findall(r'(\\d+\\.?\\d*)', response)\n",
    "    tickers = re.findall(r'\\b([A-Z]{1,5})\\b', response)\n",
    "    \n",
    "    accuracy_score = 5  # Start with perfect score\n",
    "    errors = []\n",
    "    \n",
    "    # Check if mentioned stocks have data that matches response claims\n",
    "    for ticker in tickers:\n",
    "        if ticker in stock_df['Ticker'].values:\n",
    "            stock_data = stock_df[stock_df['Ticker'] == ticker].iloc[0]\n",
    "            \n",
    "            # Check for common metric mentions\n",
    "            if 'P/E' in response or 'PE' in response:\n",
    "                if stock_data['Trailing_PE'] > 0:\n",
    "                    pe_str = f\"{stock_data['Trailing_PE']:.1f}\"\n",
    "                    if pe_str[:3] not in response:\n",
    "                        errors.append(f\"PE ratio for {ticker} may be inaccurate\")\n",
    "    \n",
    "    if errors:\n",
    "        accuracy_score = max(1, 5 - len(errors))\n",
    "    \n",
    "    return {\n",
    "        'metric': 'numerical_accuracy',\n",
    "        'score': accuracy_score,\n",
    "        'max_score': 5,\n",
    "        'reasoning': f\"Errors found: {'; '.join(errors)}\" if errors else \"No obvious numerical errors detected\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2658edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_completeness(response: str, query: str) -> Dict:\n",
    "    \"\"\"Evaluate if response adequately addresses the query.\"\"\"\n",
    "    \n",
    "    eval_prompt = f\"\"\"\n",
    "    Evaluate how completely this response addresses the original query:\n",
    "    \n",
    "    Query: {query}\n",
    "    Response: {response}\n",
    "    \n",
    "    Rate completeness on 1-5:\n",
    "    1 = Doesn't address the query at all\n",
    "    2 = Partially addresses some aspects\n",
    "    3 = Addresses main points but lacks depth\n",
    "    4 = Thoroughly addresses most aspects\n",
    "    5 = Comprehensively addresses all aspects with detail\n",
    "    \n",
    "    Provide rating in JSON: {{\"score\": <1-5>, \"reasoning\": \"<explanation>\"}}\n",
    "    \"\"\"\n",
    "    \n",
    "    evaluation = generate_groq_response(eval_prompt, max_tokens=200)\n",
    "    \n",
    "    try:\n",
    "        eval_result = json.loads(evaluation)\n",
    "        return {\n",
    "            'metric': 'response_completeness',\n",
    "            'score': eval_result.get('score', 0),\n",
    "            'max_score': 5,\n",
    "            'reasoning': eval_result.get('reasoning', 'No reasoning provided')\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            'metric': 'response_completeness',\n",
    "            'score': 0,\n",
    "            'max_score': 5,\n",
    "            'reasoning': 'Failed to parse evaluation response'\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a499a",
   "metadata": {},
   "source": [
    "##### Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc4bc025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_evaluation(test_case: Dict, rag_response: str, stock_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Run evaluation for a single test case.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'test_case': test_case,\n",
    "        'response': rag_response,\n",
    "        'evaluations': [],\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Run evaluations based on criteria\n",
    "    criteria = test_case.get('evaluation_criteria', [])\n",
    "    \n",
    "    if 'sector_relevance' in criteria and 'expected_sector' in test_case:\n",
    "        eval_result = evaluate_sector_relevance(\n",
    "            rag_response, \n",
    "            test_case['expected_sector'], \n",
    "            stock_df\n",
    "        )\n",
    "        results['evaluations'].append(eval_result)\n",
    "    \n",
    "    if 'stock_recommendations' in criteria:\n",
    "        expected_stocks = test_case.get('expected_stocks', [])\n",
    "        eval_result = evaluate_stock_recommendations(rag_response, expected_stocks)\n",
    "        results['evaluations'].append(eval_result)\n",
    "    \n",
    "    if 'financial_reasoning' in criteria:\n",
    "        eval_result = evaluate_financial_reasoning(rag_response)\n",
    "        results['evaluations'].append(eval_result)\n",
    "    \n",
    "    if 'numerical_accuracy' in criteria:\n",
    "        eval_result = evaluate_numerical_accuracy(rag_response, stock_df)\n",
    "        results['evaluations'].append(eval_result)\n",
    "    \n",
    "    # Always evaluate completeness\n",
    "    eval_result = evaluate_response_completeness(rag_response, test_case['query'])\n",
    "    results['evaluations'].append(eval_result)\n",
    "    \n",
    "    # Calculate overall score\n",
    "    if results['evaluations']:\n",
    "        scores = [eval_res['score'] for eval_res in results['evaluations']]\n",
    "        max_scores = [eval_res['max_score'] for eval_res in results['evaluations']]\n",
    "        results['overall_score'] = sum(scores) / sum(max_scores) * 100\n",
    "    else:\n",
    "        results['overall_score'] = 0\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d605466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_suite(stock_df: pd.DataFrame, sample_size: int = 20) -> List[Dict]:\n",
    "    \"\"\"Generate comprehensive test suite.\"\"\"\n",
    "    \n",
    "    test_cases = []\n",
    "    \n",
    "    # Generate different types of test cases\n",
    "    sector_tests = generate_sector_queries(stock_df)\n",
    "    performance_tests = generate_performance_queries(stock_df)\n",
    "    comparison_tests = generate_comparison_queries(stock_df)\n",
    "    portfolio_tests = generate_portfolio_queries()\n",
    "    \n",
    "    # Sample from each category\n",
    "    test_cases.extend(random.sample(sector_tests, min(8, len(sector_tests))))\n",
    "    test_cases.extend(random.sample(performance_tests, min(4, len(performance_tests))))\n",
    "    test_cases.extend(random.sample(comparison_tests, min(5, len(comparison_tests))))\n",
    "    test_cases.extend(random.sample(portfolio_tests, min(3, len(portfolio_tests))))\n",
    "    \n",
    "    return test_cases[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d3b9fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_suite(test_cases: List[Dict], rag_system_function, stock_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Run complete evaluation suite.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'summary': {\n",
    "            'total_tests': len(test_cases),\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'test_types': {}\n",
    "        },\n",
    "        'individual_results': [],\n",
    "        'aggregate_metrics': {}\n",
    "    }\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"Running test {i+1}/{len(test_cases)}: {test_case['query'][:50]}...\")\n",
    "        \n",
    "        # Get RAG system response\n",
    "        try:\n",
    "            rag_response = rag_system_function(test_case['query'])\n",
    "        except Exception as e:\n",
    "            rag_response = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Evaluate response\n",
    "        eval_result = run_single_evaluation(test_case, rag_response, stock_df)\n",
    "        results['individual_results'].append(eval_result)\n",
    "        \n",
    "        # Track test types\n",
    "        test_type = test_case.get('query_type', 'unknown')\n",
    "        if test_type not in results['summary']['test_types']:\n",
    "            results['summary']['test_types'][test_type] = 0\n",
    "        results['summary']['test_types'][test_type] += 1\n",
    "    \n",
    "    # Calculate aggregate metrics\n",
    "    all_scores = [result['overall_score'] for result in results['individual_results']]\n",
    "    results['summary']['end_time'] = datetime.now().isoformat()\n",
    "    results['summary']['average_score'] = statistics.mean(all_scores)\n",
    "    results['summary']['median_score'] = statistics.median(all_scores)\n",
    "    results['summary']['min_score'] = min(all_scores)\n",
    "    results['summary']['max_score'] = max(all_scores)\n",
    "    results['summary']['std_score'] = statistics.stdev(all_scores) if len(all_scores) > 1 else 0\n",
    "    \n",
    "    # Calculate metric-specific aggregates\n",
    "    metric_scores = {}\n",
    "    for result in results['individual_results']:\n",
    "        for evaluation in result['evaluations']:\n",
    "            metric = evaluation['metric']\n",
    "            score = evaluation['score'] / evaluation['max_score'] * 100\n",
    "            \n",
    "            if metric not in metric_scores:\n",
    "                metric_scores[metric] = []\n",
    "            metric_scores[metric].append(score)\n",
    "    \n",
    "    for metric, scores in metric_scores.items():\n",
    "        results['aggregate_metrics'][metric] = {\n",
    "            'average': statistics.mean(scores),\n",
    "            'count': len(scores),\n",
    "            'std': statistics.stdev(scores) if len(scores) > 1 else 0\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebcce1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evaluation_results(results: Dict, output_path: str):\n",
    "    \"\"\"Save evaluation results to JSON file.\"\"\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fda9434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation_summary(results: Dict):\n",
    "    \"\"\"Print formatted evaluation summary.\"\"\"\n",
    "    \n",
    "    summary = results['summary']\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RAG-LLM EVALUATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Total Tests: {summary['total_tests']}\")\n",
    "    print(f\"Average Score: {summary['average_score']:.1f}%\")\n",
    "    print(f\"Median Score: {summary['median_score']:.1f}%\")\n",
    "    print(f\"Score Range: {summary['min_score']:.1f}% - {summary['max_score']:.1f}%\")\n",
    "    print(f\"Standard Deviation: {summary['std_score']:.1f}%\")\n",
    "    \n",
    "    print(\"\\nTest Types:\")\n",
    "    for test_type, count in summary['test_types'].items():\n",
    "        print(f\"  {test_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nMetric Performance:\")\n",
    "    for metric, stats in results['aggregate_metrics'].items():\n",
    "        print(f\"  {metric}: {stats['average']:.1f}% (±{stats['std']:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nTop 3 Best Performing Tests:\")\n",
    "    sorted_results = sorted(results['individual_results'], \n",
    "                          key=lambda x: x['overall_score'], reverse=True)\n",
    "    for i, result in enumerate(sorted_results[:3]):\n",
    "        print(f\"  {i+1}. Score: {result['overall_score']:.1f}% - {result['test_case']['query'][:60]}...\")\n",
    "    \n",
    "    print(\"\\nTop 3 Worst Performing Tests:\")\n",
    "    for i, result in enumerate(sorted_results[-3:]):\n",
    "        print(f\"  {i+1}. Score: {result['overall_score']:.1f}% - {result['test_case']['query'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093eb3a",
   "metadata": {},
   "source": [
    "##### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e0c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# USAGE EXAMPLE\n",
    "# ========================\n",
    "\n",
    "def dummy_rag_system(query: str) -> str:\n",
    "    \"\"\"Dummy RAG system for testing. Replace with your actual RAG system.\"\"\"\n",
    "    return f\"This is a dummy response to: {query}\"\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your stock data\n",
    "    stock_df = load_stock_data(\"sample_data.csv\")\n",
    "    \n",
    "    # Generate test cases\n",
    "    test_cases = generate_test_suite(stock_df, sample_size=10)\n",
    "    \n",
    "    # Run evaluations (replace dummy_rag_system with your actual system)\n",
    "    results = run_evaluation_suite(test_cases, dummy_rag_system, stock_df)\n",
    "    \n",
    "    # Print and save results\n",
    "    print_evaluation_summary(results)\n",
    "    save_evaluation_results(results, \"rag_evaluation_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d0c031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
