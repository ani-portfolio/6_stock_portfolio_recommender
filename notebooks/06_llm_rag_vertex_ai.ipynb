{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from google.cloud import aiplatform_v1beta1 as aiplatform\n",
    "import uuid\n",
    "\n",
    "from tqdm import tqdm\n",
    "import yfinance as yf\n",
    "import lxml    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Annualized_Return</th>\n",
       "      <th>YTD_Pct_Return</th>\n",
       "      <th>...</th>\n",
       "      <th>2023_Pct_Return</th>\n",
       "      <th>2022_Pct_Return</th>\n",
       "      <th>2021_Pct_Return</th>\n",
       "      <th>2020_Pct_Return</th>\n",
       "      <th>Market_Cap</th>\n",
       "      <th>Pct_Diff_200_MA</th>\n",
       "      <th>Annualized_Volatility</th>\n",
       "      <th>Sharpe_Ratio</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Years_Since_Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Systems Software</td>\n",
       "      <td>Redmond, Washington</td>\n",
       "      <td>1994-06-01</td>\n",
       "      <td>789019</td>\n",
       "      <td>1975</td>\n",
       "      <td>21.32</td>\n",
       "      <td>10.40</td>\n",
       "      <td>...</td>\n",
       "      <td>58.35</td>\n",
       "      <td>-27.69</td>\n",
       "      <td>55.79</td>\n",
       "      <td>22.27</td>\n",
       "      <td>3.421644e+12</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.27</td>\n",
       "      <td>78.51</td>\n",
       "      <td>1.18</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NVDA</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Semiconductors</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>2001-11-30</td>\n",
       "      <td>1045810</td>\n",
       "      <td>1993</td>\n",
       "      <td>72.79</td>\n",
       "      <td>-2.29</td>\n",
       "      <td>...</td>\n",
       "      <td>246.10</td>\n",
       "      <td>-51.44</td>\n",
       "      <td>124.48</td>\n",
       "      <td>48.40</td>\n",
       "      <td>3.295497e+12</td>\n",
       "      <td>6.61</td>\n",
       "      <td>0.53</td>\n",
       "      <td>137.30</td>\n",
       "      <td>2.07</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc.</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Technology Hardware, Storage &amp; Peripherals</td>\n",
       "      <td>Cupertino, California</td>\n",
       "      <td>1982-11-30</td>\n",
       "      <td>320193</td>\n",
       "      <td>1977</td>\n",
       "      <td>20.76</td>\n",
       "      <td>-17.44</td>\n",
       "      <td>...</td>\n",
       "      <td>54.80</td>\n",
       "      <td>-28.20</td>\n",
       "      <td>38.06</td>\n",
       "      <td>65.49</td>\n",
       "      <td>2.999856e+12</td>\n",
       "      <td>-10.79</td>\n",
       "      <td>0.30</td>\n",
       "      <td>69.06</td>\n",
       "      <td>1.27</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Broadline Retail</td>\n",
       "      <td>Seattle, Washington</td>\n",
       "      <td>2005-11-18</td>\n",
       "      <td>1018724</td>\n",
       "      <td>1994</td>\n",
       "      <td>10.66</td>\n",
       "      <td>-6.91</td>\n",
       "      <td>...</td>\n",
       "      <td>77.04</td>\n",
       "      <td>-50.71</td>\n",
       "      <td>4.64</td>\n",
       "      <td>31.80</td>\n",
       "      <td>2.176468e+12</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.36</td>\n",
       "      <td>29.80</td>\n",
       "      <td>1.43</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>Alphabet Inc. (Class C)</td>\n",
       "      <td>Communication Services</td>\n",
       "      <td>Interactive Media &amp; Services</td>\n",
       "      <td>Mountain View, California</td>\n",
       "      <td>2006-04-03</td>\n",
       "      <td>1652044</td>\n",
       "      <td>1998</td>\n",
       "      <td>19.39</td>\n",
       "      <td>-9.22</td>\n",
       "      <td>...</td>\n",
       "      <td>57.11</td>\n",
       "      <td>-38.84</td>\n",
       "      <td>67.43</td>\n",
       "      <td>22.35</td>\n",
       "      <td>2.090085e+12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.31</td>\n",
       "      <td>62.44</td>\n",
       "      <td>1.24</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>APA</td>\n",
       "      <td>APA Corporation</td>\n",
       "      <td>Energy</td>\n",
       "      <td>Oil &amp; Gas Exploration &amp; Production</td>\n",
       "      <td>Houston, Texas</td>\n",
       "      <td>1997-07-28</td>\n",
       "      <td>1841666</td>\n",
       "      <td>1954</td>\n",
       "      <td>10.16</td>\n",
       "      <td>-25.26</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.71</td>\n",
       "      <td>69.08</td>\n",
       "      <td>83.27</td>\n",
       "      <td>21.51</td>\n",
       "      <td>6.137973e+09</td>\n",
       "      <td>-20.98</td>\n",
       "      <td>0.55</td>\n",
       "      <td>18.30</td>\n",
       "      <td>1.39</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>CZR</td>\n",
       "      <td>Caesars Entertainment</td>\n",
       "      <td>Consumer Discretionary</td>\n",
       "      <td>Casinos &amp; Gaming</td>\n",
       "      <td>Reno, Nevada</td>\n",
       "      <td>2021-03-22</td>\n",
       "      <td>1590895</td>\n",
       "      <td>1973</td>\n",
       "      <td>-5.72</td>\n",
       "      <td>-17.52</td>\n",
       "      <td>...</td>\n",
       "      <td>10.93</td>\n",
       "      <td>-55.49</td>\n",
       "      <td>30.63</td>\n",
       "      <td>105.79</td>\n",
       "      <td>5.590180e+09</td>\n",
       "      <td>-22.33</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-10.32</td>\n",
       "      <td>1.97</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>ENPH</td>\n",
       "      <td>Enphase Energy</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Semiconductor Materials &amp; Equipment</td>\n",
       "      <td>Fremont, California</td>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>1463101</td>\n",
       "      <td>2006</td>\n",
       "      <td>-7.06</td>\n",
       "      <td>-42.00</td>\n",
       "      <td>...</td>\n",
       "      <td>-47.83</td>\n",
       "      <td>43.65</td>\n",
       "      <td>6.21</td>\n",
       "      <td>194.02</td>\n",
       "      <td>5.430658e+09</td>\n",
       "      <td>-44.18</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-10.33</td>\n",
       "      <td>1.48</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>BRK.B</td>\n",
       "      <td>Berkshire Hathaway</td>\n",
       "      <td>Financials</td>\n",
       "      <td>Multi-Sector Holdings</td>\n",
       "      <td>Omaha, Nebraska</td>\n",
       "      <td>2010-02-16</td>\n",
       "      <td>1067983</td>\n",
       "      <td>1839</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>BF.B</td>\n",
       "      <td>Brown–Forman</td>\n",
       "      <td>Consumer Staples</td>\n",
       "      <td>Distillers &amp; Vintners</td>\n",
       "      <td>Louisville, Kentucky</td>\n",
       "      <td>1982-10-31</td>\n",
       "      <td>14693</td>\n",
       "      <td>1870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>503 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Ticker             Company_Name             GICS Sector  \\\n",
       "0     MSFT                Microsoft  Information Technology   \n",
       "1     NVDA                   Nvidia  Information Technology   \n",
       "2     AAPL               Apple Inc.  Information Technology   \n",
       "3     AMZN                   Amazon  Consumer Discretionary   \n",
       "4     GOOG  Alphabet Inc. (Class C)  Communication Services   \n",
       "..     ...                      ...                     ...   \n",
       "498    APA          APA Corporation                  Energy   \n",
       "499    CZR    Caesars Entertainment  Consumer Discretionary   \n",
       "500   ENPH           Enphase Energy  Information Technology   \n",
       "501  BRK.B       Berkshire Hathaway              Financials   \n",
       "502   BF.B             Brown–Forman        Consumer Staples   \n",
       "\n",
       "                              GICS Sub-Industry      Headquarters Location  \\\n",
       "0                              Systems Software        Redmond, Washington   \n",
       "1                                Semiconductors    Santa Clara, California   \n",
       "2    Technology Hardware, Storage & Peripherals      Cupertino, California   \n",
       "3                              Broadline Retail        Seattle, Washington   \n",
       "4                  Interactive Media & Services  Mountain View, California   \n",
       "..                                          ...                        ...   \n",
       "498          Oil & Gas Exploration & Production             Houston, Texas   \n",
       "499                            Casinos & Gaming               Reno, Nevada   \n",
       "500         Semiconductor Materials & Equipment        Fremont, California   \n",
       "501                       Multi-Sector Holdings            Omaha, Nebraska   \n",
       "502                       Distillers & Vintners       Louisville, Kentucky   \n",
       "\n",
       "     Date added      CIK Founded  Annualized_Return  YTD_Pct_Return  ...  \\\n",
       "0    1994-06-01   789019    1975              21.32           10.40  ...   \n",
       "1    2001-11-30  1045810    1993              72.79           -2.29  ...   \n",
       "2    1982-11-30   320193    1977              20.76          -17.44  ...   \n",
       "3    2005-11-18  1018724    1994              10.66           -6.91  ...   \n",
       "4    2006-04-03  1652044    1998              19.39           -9.22  ...   \n",
       "..          ...      ...     ...                ...             ...  ...   \n",
       "498  1997-07-28  1841666    1954              10.16          -25.26  ...   \n",
       "499  2021-03-22  1590895    1973              -5.72          -17.52  ...   \n",
       "500  2021-01-07  1463101    2006              -7.06          -42.00  ...   \n",
       "501  2010-02-16  1067983    1839                NaN             NaN  ...   \n",
       "502  1982-10-31    14693    1870                NaN             NaN  ...   \n",
       "\n",
       "     2023_Pct_Return  2022_Pct_Return  2021_Pct_Return  2020_Pct_Return  \\\n",
       "0              58.35           -27.69            55.79            22.27   \n",
       "1             246.10           -51.44           124.48            48.40   \n",
       "2              54.80           -28.20            38.06            65.49   \n",
       "3              77.04           -50.71             4.64            31.80   \n",
       "4              57.11           -38.84            67.43            22.35   \n",
       "..               ...              ...              ...              ...   \n",
       "498           -15.71            69.08            83.27            21.51   \n",
       "499            10.93           -55.49            30.63           105.79   \n",
       "500           -47.83            43.65             6.21           194.02   \n",
       "501              NaN              NaN              NaN              NaN   \n",
       "502              NaN              NaN              NaN              NaN   \n",
       "\n",
       "       Market_Cap  Pct_Diff_200_MA  Annualized_Volatility  Sharpe_Ratio  Beta  \\\n",
       "0    3.421644e+12            10.81                   0.27         78.51  1.18   \n",
       "1    3.295497e+12             6.61                   0.53        137.30  2.07   \n",
       "2    2.999856e+12           -10.79                   0.30         69.06  1.27   \n",
       "3    2.176468e+12             1.80                   0.36         29.80  1.43   \n",
       "4    2.090085e+12             0.16                   0.31         62.44  1.24   \n",
       "..            ...              ...                    ...           ...   ...   \n",
       "498  6.137973e+09           -20.98                   0.55         18.30  1.39   \n",
       "499  5.590180e+09           -22.33                   0.56        -10.32  1.97   \n",
       "500  5.430658e+09           -44.18                   0.68        -10.33  1.48   \n",
       "501           NaN              NaN                    NaN           NaN   NaN   \n",
       "502           NaN              NaN                    NaN           NaN   NaN   \n",
       "\n",
       "     Years_Since_Founded  \n",
       "0                   50.0  \n",
       "1                   32.0  \n",
       "2                   48.0  \n",
       "3                   31.0  \n",
       "4                   27.0  \n",
       "..                   ...  \n",
       "498                 71.0  \n",
       "499                 52.0  \n",
       "500                 19.0  \n",
       "501                  NaN  \n",
       "502                  NaN  \n",
       "\n",
       "[503 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sandp500 = pd.read_csv('/Users/ani/Projects/6_stock_portfolio_recommendation/data/sp500_data_sample.csv')\n",
    "df_sandp500 = df_sandp500.rename(columns={'Security': 'Company_Name'})\n",
    "df_sandp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VECTOR_SEARCH_INDEX_ID=\"projects/307012892661/locations/us-central1/indexEndpoints/240979963459665920\"\n",
    "DEPLOYED_INDEX_ID=\"stock_recommendation_app_1749700932103\"\n",
    "PROJECT_NUMBER = \"307012892661\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI initialized for project 'capable-arbor-293714' in location 'us-central1'.\n",
      "Vertex AI Generative and Embedding models loaded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "from google.cloud import aiplatform_v1 as aiplatform \n",
    "from google.cloud.aiplatform_v1.types import UpsertDatapointsRequest, IndexDatapoint, FindNeighborsRequest\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os \n",
    "\n",
    "# --- Google Cloud Configuration ---\n",
    "# Replace with your Google Cloud Project ID and Region\n",
    "PROJECT_ID = \"capable-arbor-293714\"\n",
    "LOCATION = \"us-central1\" # Or your preferred region for Vertex AI\n",
    "# IMPORTANT: Replace with your actual Google Cloud Project Number\n",
    "# You can find this in the GCP Console under \"Project info\" or \"IAM & Admin\" -> \"Settings\".\n",
    "PROJECT_NUMBER = \"307012892661\"\n",
    "\n",
    "# --- Vertex AI Vector Search Configuration ---\n",
    "# IMPORTANT: Replace with your actual Vertex AI Vector Search Index ID and Deployed Index ID\n",
    "# You need to have created and deployed a Vector Search index in your project.\n",
    "VECTOR_SEARCH_INDEX_ID=\"2810072444642000896\"\n",
    "ENDPOINT_ID=\"240979963459665920\"\n",
    "\n",
    "# Dynamically construct the specific API endpoint for the MatchServiceClient (for querying)\n",
    "# This format uses the Project Number for the VDB endpoint.\n",
    "# API_ENDPOINT_FOR_MATCH_CLIENT = f\"{ENDPOINT_ID}.{LOCATION}-{PROJECT_NUMBER}.vdb.vertexai.goog\"\n",
    "API_ENDPOINT_FOR_MATCH_CLIENT=\"801674555.us-central1-307012892661.vdb.vertexai.goog\"\n",
    "\n",
    "# Initialize Vertex AI\n",
    "try:\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "    print(f\"Vertex AI initialized for project '{PROJECT_ID}' in location '{LOCATION}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize Vertex AI. Please ensure your project ID and location are correct and you are authenticated. Error: {e}\")\n",
    "    exit() \n",
    "\n",
    "# Initialize Vertex AI models\n",
    "try:\n",
    "    # Using \"gemini-2.0-flash\". If you continue to get 404 errors,\n",
    "    # please verify the exact model name and its availability in your region\n",
    "    # (e.g., \"gemini-pro\", \"gemini-1.5-pro-001\") via Google Cloud documentation.\n",
    "    generation_model = GenerativeModel(\"gemini-2.0-flash\") \n",
    "    embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
    "    print(\"Vertex AI Generative and Embedding models loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to load Vertex AI models. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Initialize Vertex AI Vector Search clients\n",
    "try:\n",
    "    # Client for managing and uploading data to the index (uses standard regional endpoint)\n",
    "    index_service_client = aiplatform.IndexServiceClient(client_options={\"api_endpoint\": f\"{LOCATION}-aiplatform.googleapis.com\"})\n",
    "    # Client for querying the deployed index (uses the specific VDB endpoint)\n",
    "    match_service_client = aiplatform.MatchServiceClient(client_options={\"api_endpoint\": API_ENDPOINT_FOR_MATCH_CLIENT})\n",
    "    print(\"Vertex AI Vector Search clients initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Failed to initialize Vertex AI Vector Search clients. Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- RAG Components ---\n",
    "\n",
    "def get_langchain_text_chunks(text, chunk_size=1000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks using LangChain's RecursiveCharacterTextSplitter.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Create a LangChain Document object from the text\n",
    "    document = Document(page_content=text)\n",
    "    \n",
    "    # Initialize the text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len, # Use standard len for character count\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = text_splitter.split_documents([document])\n",
    "    \n",
    "    # Return the content of the chunks\n",
    "    return [chunk.page_content for chunk in chunks]\n",
    "\n",
    "\n",
    "def process_dataframe_for_vector_search(df: pd.DataFrame, text_columns: list):\n",
    "    \"\"\"\n",
    "    Processes a Pandas DataFrame, creates textual chunks from specified columns using LangChain,\n",
    "    generates embeddings, and upserts them to Vertex AI Vector Search.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame (e.g., df_sandp500).\n",
    "        text_columns (list): A list of column names whose content should be combined\n",
    "                             to form the text for embedding.\n",
    "    Returns:\n",
    "        dict: An in-memory dictionary storing text chunks keyed by their unique IDs.\n",
    "    \"\"\"\n",
    "    # Initialize document_store locally within this function\n",
    "    document_store = {} \n",
    "\n",
    "    print(f\"Processing DataFrame with {len(df)} rows and columns: {text_columns}\")\n",
    "    all_datapoints = []\n",
    "    processed_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Combine specified columns into a single text string for RAG\n",
    "        combined_text = \" \".join(str(row[col]) for col in text_columns if col in row)\n",
    "        \n",
    "        # Use LangChain to get text chunks\n",
    "        chunks = get_langchain_text_chunks(combined_text)\n",
    "        if not chunks:\n",
    "            print(f\"WARNING: No text extracted or chunks created from row {index}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Generate embeddings for the chunks (batching for efficiency)\n",
    "        embeddings_for_row_chunks = []\n",
    "        for i in range(0, len(chunks), 5): # Batch requests to avoid API limits\n",
    "            batch_chunks = chunks[i:i+5]\n",
    "            try:\n",
    "                # Assuming get_embeddings returns a list of Embedding objects directly\n",
    "                response = embedding_model.get_embeddings(batch_chunks)\n",
    "                for j, emb_item in enumerate(response): \n",
    "                    embeddings_for_row_chunks.append((batch_chunks[j], emb_item.values))\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: Error generating embeddings for a batch from row {index}: {e}. Skipping this batch.\")\n",
    "                continue\n",
    "\n",
    "        if not embeddings_for_row_chunks:\n",
    "            print(f\"WARNING: No embeddings could be generated for row {index}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare IndexDatums for Vector Search and populate document_store\n",
    "        for chunk, embedding_vector in embeddings_for_row_chunks:\n",
    "            data_id = str(uuid.uuid4()) # Unique ID for each data point\n",
    "            \n",
    "            # Store the actual text chunk in our in-memory document store\n",
    "            document_store[data_id] = chunk\n",
    "            print(f\"Storing chunk with ID {data_id} in document store.\")\n",
    "\n",
    "            datapoint = IndexDatapoint(\n",
    "                datapoint_id=data_id,\n",
    "                feature_vector=embedding_vector\n",
    "            )\n",
    "            all_datapoints.append(datapoint)\n",
    "        processed_count += 1\n",
    "        if processed_count % 100 == 0:\n",
    "            print(f\"Processed {processed_count} rows, generated {len(all_datapoints)} data points so far.\")\n",
    "\n",
    "    # Upsert all collected datapoints to Vertex AI Vector Search\n",
    "    if not all_datapoints:\n",
    "        print(\"No data points to upsert to Vector Search.\")\n",
    "        return document_store\n",
    "\n",
    "    try:\n",
    "        index_resource_name = index_service_client.index_path(PROJECT_ID, LOCATION, VECTOR_SEARCH_INDEX_ID)\n",
    "        \n",
    "        # Vertex AI Vector Search has limits on the number of datapoints per upsert call.\n",
    "        # It's good practice to batch them if you have many.\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(all_datapoints), batch_size):\n",
    "            batch_datapoints = all_datapoints[i:i + batch_size]\n",
    "            print(f\"Upserting batch {i//batch_size + 1}/{(len(all_datapoints) + batch_size - 1)//batch_size} ({len(batch_datapoints)} datapoints)...\")\n",
    "            \n",
    "            # Construct the request object\n",
    "            request = UpsertDatapointsRequest(\n",
    "                index=index_resource_name,\n",
    "                datapoints=batch_datapoints\n",
    "            )\n",
    "            \n",
    "            # Pass the request object as a keyword argument\n",
    "            response = index_service_client.upsert_datapoints(request=request)\n",
    "            # Removed direct access to .name due to observed error; assuming success if no exception\n",
    "            print(f\"Batch upserted successfully.\") \n",
    "        \n",
    "        print(f\"Successfully upserted a total of {len(all_datapoints)} data points to Vector Search.\")\n",
    "        print(\"Note: It may take some time for the changes to propagate and be searchable in Vector Search.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error upserting data to Vertex AI Vector Search: {e}\")\n",
    "        print(\"Please ensure your Vector Search Index and Endpoint are correctly configured and deployed.\")\n",
    "    \n",
    "    return document_store # Return the populated document_store\n",
    "\n",
    "\n",
    "def retrieve_relevant_documents(query, document_store, top_k=3): # Added document_store as argument\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant document chunks from Vertex AI Vector Search.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding_response = embedding_model.get_embeddings([query])\n",
    "        query_embedding = query_embedding_response[0].values \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error generating query embedding: {e}\")\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Construct the full index endpoint resource name (used for index_endpoint in FindNeighborsRequest)\n",
    "        index_endpoint_resource_name = ( \n",
    "            f\"projects/{PROJECT_NUMBER}/locations/{LOCATION}/indexEndpoints/{ENDPOINT_ID}\"\n",
    "        )\n",
    "\n",
    "        # Build FindNeighborsRequest object as shown in the provided snippet\n",
    "        datapoint = IndexDatapoint(\n",
    "            feature_vector=query_embedding\n",
    "        )\n",
    "\n",
    "        query_obj = FindNeighborsRequest.Query(\n",
    "            datapoint=datapoint,\n",
    "            neighbor_count=top_k # The number of nearest neighbors to be retrieved\n",
    "        )\n",
    "\n",
    "        # Construct the FindNeighborsRequest with keyword arguments as expected by v1\n",
    "        request = FindNeighborsRequest(\n",
    "            index_endpoint=index_endpoint_resource_name,\n",
    "            deployed_index_id=\"stock_recommendation_app_1749700932103\",\n",
    "            queries=[query_obj],\n",
    "            return_full_datapoint=False,\n",
    "        )\n",
    "\n",
    "        # Execute the request\n",
    "        response = match_service_client.find_neighbors(request=request)\n",
    "\n",
    "        relevant_chunks = []\n",
    "        print(\"\\n--- Retrieved Neighbors (for debugging) ---\")\n",
    "        for neighbor in response.nearest_neighbors[0].neighbors:\n",
    "            # Use the datapoint_id to retrieve the original text chunk from our document_store\n",
    "            chunk_id = neighbor.datapoint.datapoint_id\n",
    "            print(f\"Chunk ID: {chunk_id}\")\n",
    "            chunk_text = document_store.get(chunk_id, \"Error: Chunk text not found in store.\")\n",
    "            \n",
    "            if chunk_text:\n",
    "                relevant_chunks.append(chunk_text)\n",
    "                print(f\"  - Similarity: {neighbor.distance:.4f}, Chunk ID: {chunk_id}, Chunk: \\\"{chunk_text[:100]}...\\\"\")\n",
    "        print(\"-------------------------------------------\\n\")\n",
    "        return relevant_chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error retrieving from Vertex AI Vector Search: {e}\")\n",
    "        print(\"Please ensure your Vector Search Index and Endpoint are correctly configured and deployed.\")\n",
    "        return []\n",
    "\n",
    "def generate_response_with_rag(user_query, document_store): # Added document_store as argument\n",
    "    \"\"\"Generates a response using retrieved context and Gemini.\"\"\"\n",
    "    relevant_docs = retrieve_relevant_documents(user_query, document_store) # Passed document_store\n",
    "\n",
    "    if not relevant_docs:\n",
    "        print(\"WARNING: Could not find relevant documents in the knowledge base. Generating response based on general knowledge.\")\n",
    "        context_text = \"\"\n",
    "    else:\n",
    "        context_text = \"\\n\\n\".join(relevant_docs)\n",
    "        print(\"\\n--- Context provided to LLM ---\")\n",
    "        print(context_text)\n",
    "        print(\"-------------------------------\\n\")\n",
    "\n",
    "    # Construct the prompt with context\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Answer the user's question truthfully based on the provided context.\n",
    "    If the answer is not available in the context, state that you cannot find the answer in the provided information.\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question: {user_query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = generation_model.generate_content(\n",
    "            [Part.from_text(prompt)],\n",
    "            generation_config={\"temperature\": 0.2}\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error generating content with Gemini: {e}\")\n",
    "        return \"An error occurred while generating the response.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting RAG Notebook Script ---\n",
      "\n",
      "Dummy df_sandp500 created:\n",
      "  Ticker             Company_Name  Annualized_Return  Market_Cap_B  \\\n",
      "0   AAPL               Apple Inc.               0.25          3000   \n",
      "1   MSFT          Microsoft Corp.               0.30          2500   \n",
      "2  GOOGL  Alphabet Inc. (Class A)               0.20          2000   \n",
      "3   AMZN          Amazon.com Inc.               0.18          1800   \n",
      "4   TSLA               Tesla Inc.               0.40           800   \n",
      "\n",
      "                   Sector                                        Description  \n",
      "0              Technology  Apple Inc. designs, manufactures, and markets ...  \n",
      "1              Technology  Microsoft Corporation develops, licenses, and ...  \n",
      "2  Communication Services  Alphabet Inc. provides various products and se...  \n",
      "3  Consumer Discretionary  Amazon.com, Inc. engages in the retail sale of...  \n",
      "4  Consumer Discretionary  Tesla, Inc. designs, develops, manufactures, l...  \n",
      "Processing DataFrame with 5 rows and columns: ['Ticker', 'Company_Name', 'Annualized_Return', 'Sector', 'Description']\n",
      "Upserting batch 1/1 (5 datapoints)...\n",
      "Batch upserted successfully.\n",
      "Successfully upserted a total of 5 data points to Vector Search.\n",
      "Note: It may take some time for the changes to propagate and be searchable in Vector Search.\n",
      "\n",
      "Waiting a few seconds for Vector Search changes to propagate...\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage in a Jupyter Notebook ---\n",
    "print(\"--- Starting RAG Notebook Script ---\")\n",
    "\n",
    "# 1. Create a dummy DataFrame for demonstration\n",
    "# In your actual notebook, df_sandp500 would be loaded from a file or database\n",
    "data = {\n",
    "    'Ticker': ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA'],\n",
    "    'Company_Name': ['Apple Inc.', 'Microsoft Corp.', 'Alphabet Inc. (Class A)', 'Amazon.com Inc.', 'Tesla Inc.'],\n",
    "    'Annualized_Return': [0.25, 0.30, 0.20, 0.18, 0.40],\n",
    "    'Market_Cap_B': [3000, 2500, 2000, 1800, 800],\n",
    "    'Sector': ['Technology', 'Technology', 'Communication Services', 'Consumer Discretionary', 'Consumer Discretionary'],\n",
    "    'Description': [\n",
    "        \"Apple Inc. designs, manufactures, and markets smartphones, personal computers, tablets, wearables, and accessories worldwide. It also sells related services.\",\n",
    "        \"Microsoft Corporation develops, licenses, and supports software, services, devices, and solutions worldwide. Its segments include Productivity and Business Processes, Intelligent Cloud, and More Personal Computing.\",\n",
    "        \"Alphabet Inc. provides various products and services worldwide. It operates through Google Services, Google Cloud, and Other Bets segments.\",\n",
    "        \"Amazon.com, Inc. engages in the retail sale of consumer products and subscriptions in North America and internationally. It operates through North America, International, and Amazon Web Services (AWS) segments.\",\n",
    "        \"Tesla, Inc. designs, develops, manufactures, leases, and sells electric vehicles, and energy generation and storage systems in the United States, China, and internationally.\"\n",
    "    ]\n",
    "}\n",
    "df_sandp500 = pd.DataFrame(data)\n",
    "print(\"\\nDummy df_sandp500 created:\")\n",
    "print(df_sandp500.head())\n",
    "\n",
    "# 2. Process the DataFrame and upsert to Vector Search\n",
    "# The document_store is now returned by this function\n",
    "columns_to_embed = ['Ticker', 'Company_Name', 'Annualized_Return', 'Sector', 'Description']\n",
    "# Store the returned document_store in a local variable\n",
    "my_document_store = process_dataframe_for_vector_search(df_sandp500, columns_to_embed)\n",
    "\n",
    "# Give some time for Vector Search changes to propagate if running immediately after upsert\n",
    "print(\"\\nWaiting a few seconds for Vector Search changes to propagate...\")\n",
    "import time\n",
    "time.sleep(10) # Adjust as needed based on your index configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing RAG Queries ---\n",
      "\n",
      "User Query: What is the annualized return for Apple?\n",
      "\n",
      "--- Retrieved Neighbors (for debugging) ---\n",
      "  - Similarity: 0.6347, Chunk ID: 673e4c20-883b-4b7b-8984-e26ce37468d4, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.6347, Chunk ID: 553558fd-28b7-4254-8e88-0c56fe406c4b, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.6347, Chunk ID: 09bfc957-1a7e-4619-8a6d-e8fa3df8da83, Chunk: \"Error: Chunk text not found in store....\"\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "--- Context provided to LLM ---\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "-------------------------------\n",
      "\n",
      "Assistant Response: I am unable to answer the question. The provided context contains only the error message \"Error: Chunk text not found in store.\" and does not contain any information about Apple's annualized return.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: Tell me about Microsoft's main business segments.\n",
      "\n",
      "--- Retrieved Neighbors (for debugging) ---\n",
      "  - Similarity: 0.6781, Chunk ID: d094770d-1e77-4a60-91e9-052a053324b0, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.6781, Chunk ID: 50c19e42-b5b1-4b14-ab9b-7edbc6bee48e, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.6781, Chunk ID: e01041f7-8e4f-4abc-86f1-d55721087551, Chunk: \"Error: Chunk text not found in store....\"\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "--- Context provided to LLM ---\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "-------------------------------\n",
      "\n",
      "Assistant Response: I am unable to answer your question about Microsoft's main business segments. The provided context consists only of error messages: \"Error: Chunk text not found in store.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: Which companies are in the Consumer Discretionary sector?\n",
      "\n",
      "--- Retrieved Neighbors (for debugging) ---\n",
      "  - Similarity: 0.5764, Chunk ID: c1f14a1b-97f1-4223-9f7e-926e0ba26e63, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.5764, Chunk ID: e09a6266-ac98-4541-8445-ffe87be531bb, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.5764, Chunk ID: b6c50358-ad18-43a2-b3cc-0b05178586ec, Chunk: \"Error: Chunk text not found in store....\"\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "--- Context provided to LLM ---\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "-------------------------------\n",
      "\n",
      "Assistant Response: I am unable to answer the question about which companies are in the Consumer Discretionary sector. The provided context contains only error messages: \"Error: Chunk text not found in store.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: What is the market cap of Google?\n",
      "\n",
      "--- Retrieved Neighbors (for debugging) ---\n",
      "  - Similarity: 0.5999, Chunk ID: 8561f87e-194f-4f1f-a439-d6235b30cf45, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.5999, Chunk ID: 154115df-9bcc-47c4-9dd9-c67e2072580e, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.5999, Chunk ID: 164544cf-4ab1-47f0-ad80-6eb03f33594c, Chunk: \"Error: Chunk text not found in store....\"\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "--- Context provided to LLM ---\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "-------------------------------\n",
      "\n",
      "Assistant Response: I am unable to answer the question about Google's market cap because the provided context does not contain that information.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "User Query: What is the capital of France?\n",
      "\n",
      "--- Retrieved Neighbors (for debugging) ---\n",
      "  - Similarity: 0.3276, Chunk ID: 85c66c2f-63f9-4f15-b703-80cbcb83dab1, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.3276, Chunk ID: 673e4c20-883b-4b7b-8984-e26ce37468d4, Chunk: \"Error: Chunk text not found in store....\"\n",
      "  - Similarity: 0.3276, Chunk ID: e2f40b3c-6147-4c72-b890-53c3f8acb67c, Chunk: \"Error: Chunk text not found in store....\"\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "--- Context provided to LLM ---\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "\n",
      "Error: Chunk text not found in store.\n",
      "-------------------------------\n",
      "\n",
      "Assistant Response: I cannot find the answer in the provided information.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "--- RAG Notebook Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "# 3. Test the RAG system with queries\n",
    "print(\"\\n--- Testing RAG Queries ---\")\n",
    "queries = [\n",
    "    \"What is the annualized return for Apple?\",\n",
    "    \"Tell me about Microsoft's main business segments.\",\n",
    "    \"Which companies are in the Consumer Discretionary sector?\",\n",
    "    \"What is the market cap of Google?\", \n",
    "    \"What is the capital of France?\" \n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nUser Query: {query}\")\n",
    "    # Pass my_document_store to generate_response_with_rag\n",
    "    response = generate_response_with_rag(query, my_document_store) \n",
    "    print(f\"Assistant Response: {response}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"\\n--- RAG Notebook Script Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
