{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e8ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from pinecone import Pinecone\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992e84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api = open(\"/Users/ani/Documents/0_API_KEYS/groq.txt\").read().strip()\n",
    "pinecone_api = open(\"/Users/ani/Documents/0_API_KEYS/pinecone.txt\").read().strip()\n",
    "groq_llm_model = \"llama-3.3-70b-versatile\"\n",
    "huggingface_embeddings_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "pinecone_index_name = \"stock-recommendation-app-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0522f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rag_components(pinecone_api: str, \n",
    "                              groq_api: str, \n",
    "                              groq_llm_model: str, \n",
    "                              huggingface_embeddings_model: str,\n",
    "                              pinecone_index_name: str) -> tuple:\n",
    "    \"\"\"Initialize all RAG components: Pinecone, Groq LLM, and HuggingFace embeddings\"\"\"\n",
    "    \n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=pinecone_api)\n",
    "    index = pc.Index(pinecone_index_name)\n",
    "\n",
    "    # Initialize Groq LLM\n",
    "    llm = ChatGroq(\n",
    "        groq_api_key=groq_api,\n",
    "        model=groq_llm_model,\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    # Initialize HuggingFace embeddings (same as used in indexing)\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=huggingface_embeddings_model,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    return index, llm, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15405771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_context(query: str, index, embeddings, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Retrieve relevant stock data from Pinecone\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert query to embedding\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        \n",
    "        # Search Pinecone for similar vectors\n",
    "        search_results = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            include_values=False\n",
    "        )\n",
    "        \n",
    "        # Extract relevant documents\n",
    "        retrieved_docs = []\n",
    "        for match in search_results.matches:\n",
    "            retrieved_docs.append({\n",
    "                'score': match.score,\n",
    "                'content': match.metadata.get('content', 'Content not available'),\n",
    "                'metadata': {\n",
    "                    'Ticker': match.metadata.get('Ticker'),\n",
    "                    'Company_Name': match.metadata.get('Company_Name'),\n",
    "                    'Sector': match.metadata.get('Sector'),\n",
    "                    'Industry': match.metadata.get('Industry'),\n",
    "                },\n",
    "                'id': match.id\n",
    "            })\n",
    "        \n",
    "        return retrieved_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving context: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03fc7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_llm(retrieved_docs: List[Dict]) -> str:\n",
    "    \"\"\"Format retrieved documents for LLM context\"\"\"\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant stock data found.\"\n",
    "    \n",
    "    context_parts = []\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        context_parts.append(f\"=== Stock Data {i} (Relevance: {doc['score']:.3f}) ===\")\n",
    "        context_parts.append(doc['content'])  # This contains all the formatted stock info\n",
    "        context_parts.append(\"\")  # Spacing\n",
    "    \n",
    "    return \"\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba2b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stock_prompt_template():\n",
    "    \"\"\"Create a prompt template for stock-related queries\"\"\"\n",
    "    \n",
    "    template = \"\"\"\n",
    "You are a knowledgeable financial advisor assistant. Based on the stock data provided below, answer the user's question accurately and concisely.\n",
    "\n",
    "CONTEXT (Retrieved Stock Data):\n",
    "{context}\n",
    "\n",
    "USER QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer based ONLY on the provided stock data\n",
    "2. If the information isn't available in the context, clearly state that\n",
    "3. Provide specific numbers when available (percentages, dollar amounts, etc.)\n",
    "4. If multiple stocks are relevant, compare them\n",
    "5. Be conversational but professional\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    \n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c41f51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query_stocks(query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced RAG function using stored page content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        index, llm, embeddings = initialize_rag_components(\n",
    "            pinecone_api=pinecone_api,\n",
    "            groq_api=groq_api,\n",
    "            groq_llm_model=groq_llm_model,\n",
    "            huggingface_embeddings_model=huggingface_embeddings_model,\n",
    "            pinecone_index_name=pinecone_index_name\n",
    "        )\n",
    "        \n",
    "        # Use enhanced retrieval with stored content\n",
    "        print(\"Retrieving relevant stock data with full content...\")\n",
    "        retrieved_docs = retrieve_relevant_context(query, index, embeddings, top_k)\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I couldn't find any relevant stock data for your query. Please check if the stock is in our database.\",\n",
    "                \"context\": [],\n",
    "                \"query\": query,\n",
    "                \"success\": False\n",
    "            }\n",
    "        \n",
    "        # Format context using full content\n",
    "        context_text = format_context_for_llm(retrieved_docs)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt_template = create_stock_prompt_template()\n",
    "        formatted_prompt = prompt_template.format(\n",
    "            context=context_text,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Generate answer using Groq\n",
    "        print(\"Generating answer with Groq...\")\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        answer = response.content\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"context\": retrieved_docs,\n",
    "            \"query\": query,\n",
    "            \"num_sources\": len(retrieved_docs),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
    "            \"context\": [],\n",
    "            \"query\": query,\n",
    "            \"success\": False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c2e17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1: Basic Query ===\n",
      "Query: What is the annualized return of Apple?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ani/Projects/6_stock_portfolio_recommendation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving relevant stock data with full content...\n",
      "Generating answer with Groq...\n",
      "Answer: The annualized return of Apple (AAPL) is 18.35%. This information is available in the provided stock data for Apple. If you'd like to compare this to other stocks, the annualized returns for the other companies are: KeyCorp (KEY) at 8.58%, Workday, Inc. (WDAY) at 6.12%, Monster Beverage (MNST) at 12.94%, and Microsoft (MSFT) at 20.82%.\n",
      "Sources: 5\n",
      "\n",
      "=== Example 2: Comparison Query ===\n",
      "Query: What is the annualized return of NVIDIA?\n",
      "Retrieving relevant stock data with full content...\n",
      "Generating answer with Groq...\n",
      "Answer: The annualized return of NVIDIA (NVDA) is 73.49%. This is significantly higher than the other stocks listed, with the next closest being Microsoft (MSFT) at 20.82% and Netflix (NFLX) at 22.3%. In comparison, the annualized returns of F5, Inc. (FFIV) and Northrop Grumman (NOC) are 15.67% and 10.9%, respectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage functions\n",
    "def main_examples():\n",
    "    \"\"\"Example usage of the RAG functions\"\"\"\n",
    "    \n",
    "    # Example 1: Basic query\n",
    "    print(\"=== Example 1: Basic Query ===\")\n",
    "    result1 = rag_query_stocks(\"What is the annualized return of Apple?\")\n",
    "    print(f\"Answer: {result1['answer']}\")\n",
    "    print(f\"Sources: {result1['num_sources']}\")\n",
    "    print()\n",
    "    \n",
    "    # Example 2: Comparison query\n",
    "    print(\"=== Example 2: Comparison Query ===\")\n",
    "    result2 = rag_query_stocks(\"What is the annualized return of NVIDIA?\")\n",
    "    print(f\"Answer: {result2['answer']}\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c3f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8405b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
