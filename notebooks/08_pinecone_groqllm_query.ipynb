{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e8ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from pinecone import Pinecone\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api = \"\"\n",
    "groq_api = \"\"\n",
    "groq_llm_model = \"llama-3.3-70b-versatile\"\n",
    "huggingface_embeddings_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "pinecone_index_name = \"stock-data-index-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0522f8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_rag_components(pinecone_api: str, \n",
    "                              groq_api: str, \n",
    "                              groq_llm_model: str, \n",
    "                              huggingface_embeddings_model: str,\n",
    "                              pinecone_index_name: str) -> tuple:\n",
    "    \"\"\"Initialize all RAG components: Pinecone, Groq LLM, and HuggingFace embeddings\"\"\"\n",
    "    \n",
    "    # Initialize Pinecone\n",
    "    pc = Pinecone(api_key=pinecone_api)\n",
    "    index = pc.Index(pinecone_index_name)\n",
    "\n",
    "    # Initialize Groq LLM\n",
    "    llm = ChatGroq(\n",
    "        groq_api_key=groq_api,\n",
    "        model=groq_llm_model,\n",
    "        temperature=0.1,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "    \n",
    "    # Initialize HuggingFace embeddings (same as used in indexing)\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=huggingface_embeddings_model,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    \n",
    "    return index, llm, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15405771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_context(query: str, index, embeddings, top_k: int = 5) -> List[Dict]:\n",
    "    \"\"\"Retrieve relevant stock data from Pinecone\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Convert query to embedding\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        \n",
    "        # Search Pinecone for similar vectors\n",
    "        search_results = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            include_values=False\n",
    "        )\n",
    "        \n",
    "        # Extract relevant documents\n",
    "        retrieved_docs = []\n",
    "        for match in search_results.matches:\n",
    "            retrieved_docs.append({\n",
    "                'score': match.score,\n",
    "                'content': match.metadata.get('content', 'Content not available'),\n",
    "                'metadata': {\n",
    "                    'ticker': match.metadata.get('ticker'),\n",
    "                    'company_name': match.metadata.get('company_name'),\n",
    "                    'sector': match.metadata.get('sector'),\n",
    "                    'industry': match.metadata.get('industry'),\n",
    "                    'update_date': match.metadata.get('update_date')\n",
    "                },\n",
    "                'id': match.id\n",
    "            })\n",
    "        \n",
    "        return retrieved_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving context: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03fc7d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_context_for_llm(retrieved_docs: List[Dict]) -> str:\n",
    "    \"\"\"Format retrieved documents for LLM context\"\"\"\n",
    "    \n",
    "    if not retrieved_docs:\n",
    "        return \"No relevant stock data found.\"\n",
    "    \n",
    "    context_parts = []\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        context_parts.append(f\"=== Stock Data {i} (Relevance: {doc['score']:.3f}) ===\")\n",
    "        context_parts.append(doc['content'])  # This contains all the formatted stock info\n",
    "        context_parts.append(\"\")  # Spacing\n",
    "    \n",
    "    return \"\\n\".join(context_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ba2b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stock_prompt_template():\n",
    "    \"\"\"Create a prompt template for stock-related queries\"\"\"\n",
    "    \n",
    "    template = \"\"\"\n",
    "You are a knowledgeable financial advisor assistant. Based on the stock data provided below, answer the user's question accurately and concisely.\n",
    "\n",
    "CONTEXT (Retrieved Stock Data):\n",
    "{context}\n",
    "\n",
    "USER QUESTION: {question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer based ONLY on the provided stock data\n",
    "2. If the information isn't available in the context, clearly state that\n",
    "3. Provide specific numbers when available (percentages, dollar amounts, etc.)\n",
    "4. If multiple stocks are relevant, compare them\n",
    "5. Be conversational but professional\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    \n",
    "    return PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c41f51c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query_stocks(query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"Enhanced RAG function using stored page content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Initialize components\n",
    "        index, llm, embeddings = initialize_rag_components(\n",
    "            pinecone_api=pinecone_api,\n",
    "            groq_api=groq_api,\n",
    "            groq_llm_model=groq_llm_model,\n",
    "            huggingface_embeddings_model=huggingface_embeddings_model,\n",
    "            pinecone_index_name=pinecone_index_name\n",
    "        )\n",
    "        \n",
    "        # Use enhanced retrieval with stored content\n",
    "        print(\"Retrieving relevant stock data with full content...\")\n",
    "        retrieved_docs = retrieve_relevant_context(query, index, embeddings, top_k)\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                \"answer\": \"I couldn't find any relevant stock data for your query. Please check if the stock is in our database.\",\n",
    "                \"context\": [],\n",
    "                \"query\": query,\n",
    "                \"success\": False\n",
    "            }\n",
    "        \n",
    "        # Format context using full content\n",
    "        context_text = format_context_for_llm(retrieved_docs)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt_template = create_stock_prompt_template()\n",
    "        formatted_prompt = prompt_template.format(\n",
    "            context=context_text,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        # Generate answer using Groq\n",
    "        print(\"Generating answer with Groq...\")\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        answer = response.content\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"context\": retrieved_docs,\n",
    "            \"query\": query,\n",
    "            \"num_sources\": len(retrieved_docs),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"An error occurred while processing your query: {str(e)}\",\n",
    "            \"context\": [],\n",
    "            \"query\": query,\n",
    "            \"success\": False\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8c2e17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1: Basic Query ===\n",
      "Query: What is the annualized return of Apple?\n",
      "Retrieving relevant stock data with full content...\n",
      "Generating answer with Groq...\n",
      "Answer: The annualized return of Apple (AAPL) is 20.76%. This is based on the provided stock data for Apple Inc. If you'd like to compare this to other stocks, the closest annualized return is ServiceNow (NOW) at 21.4% and IBM at 22.06%, while the other stocks have lower annualized returns.\n",
      "Sources: 5\n",
      "\n",
      "=== Example 2: Comparison Query ===\n",
      "Query: What is the annualized return of NVIDIA?\n",
      "Retrieving relevant stock data with full content...\n",
      "Generating answer with Groq...\n",
      "Answer: The annualized return of NVIDIA (NVDA) is 72.79%. This is significantly higher than the other stocks listed, with the next closest being Netflix (NFLX) at 23.17%. If you're looking for a comparison, NVIDIA's annualized return is roughly three times that of Netflix, indicating a much stronger historical performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage functions\n",
    "def main_examples():\n",
    "    \"\"\"Example usage of the RAG functions\"\"\"\n",
    "    \n",
    "    # Example 1: Basic query\n",
    "    print(\"=== Example 1: Basic Query ===\")\n",
    "    result1 = rag_query_stocks(\"What is the annualized return of Apple?\")\n",
    "    print(f\"Answer: {result1['answer']}\")\n",
    "    print(f\"Sources: {result1['num_sources']}\")\n",
    "    print()\n",
    "    \n",
    "    # Example 2: Comparison query\n",
    "    print(\"=== Example 2: Comparison Query ===\")\n",
    "    result2 = rag_query_stocks(\"What is the annualized return of NVIDIA?\")\n",
    "    print(f\"Answer: {result2['answer']}\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c3f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8405b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
