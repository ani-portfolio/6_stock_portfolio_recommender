{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e25c317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import flow, task, get_run_logger\n",
    "from prefect.blocks.system import Secret\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047ea249",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone_api = open(\"/Users/ani/Documents/0_API_KEYS/pinecone.txt\").read().strip()\n",
    "huggingface_embeddings_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "pinecone_index_name = \"stock-recommendation-app-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45715ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_pinecone(pinecone_api):\n",
    "    \"\"\"Initialize Pinecone connection and return client\"\"\"    \n",
    "    try:        \n",
    "        # Initialize Pinecone client\n",
    "        pc = Pinecone(api_key=pinecone_api)\n",
    "        \n",
    "        return pc\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2b8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_huggingface_embeddings(huggingface_embeddings_model):\n",
    "    \"\"\"Initialize HuggingFace embedding model\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize HuggingFace embeddings (no API key required)\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=huggingface_embeddings_model,\n",
    "            model_kwargs={'device': 'cpu'},  # Use 'cuda' if GPU available\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "289712e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_chunks(df):\n",
    "    \"\"\"Create text chunks from stock data for embedding\"\"\"\n",
    "    \n",
    "    try:\n",
    "        documents = []\n",
    "        columns = df.columns.tolist()\n",
    "        \n",
    "        # Convert each stock record to a text document\n",
    "        for _, row in df.iterrows():\n",
    "            doc_content = \"\"\n",
    "            for col in columns:\n",
    "                if pd.notna(row[col]):\n",
    "                    doc_content += f\"{col}: {row[col]}\\n\"\n",
    "            \n",
    "            # Create metadata for each document (FIXED: removed nested \"metadata\" key)\n",
    "            metadata = {\n",
    "                \"Ticker\": row['Ticker'],\n",
    "                \"Company_Name\": row['Company_Name'], \n",
    "                \"Sector\": row['Sector'],\n",
    "                \"Industry\": row['Industry'],\n",
    "            }\n",
    "\n",
    "            documents.append(Document(page_content=doc_content.strip(), metadata=metadata))\n",
    "        \n",
    "        # Initialize text splitter for chunking  \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Better separators for structured data\n",
    "        )\n",
    "        \n",
    "        # Split documents into chunks\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "        print(f\"Created {len(chunks)} chunks from {len(df)} stock records\")\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating text chunks: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bce00135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_with_model(chunks, embeddings_model):\n",
    "    \"\"\"Create embeddings for text chunks using the embedding model\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Extract text content from chunks\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "        \n",
    "        # Create embeddings\n",
    "        embeddings = embeddings_model.embed_documents(texts)\n",
    "\n",
    "        print(f\"Successfully created embeddings for {len(texts)} chunks\")\n",
    "        return embeddings\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create embeddings: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33babc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_to_pinecone(pc, chunks, embeddings, index_name, clear_existing):\n",
    "    \"\"\"Save vector embeddings to Pinecone with metadata and page content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        existing_indexes = pc.list_indexes().names()\n",
    "        \n",
    "        if index_name not in existing_indexes:\n",
    "            print(f\"Creating new Pinecone index: {index_name}\")\n",
    "            pc.create_index(\n",
    "                name=index_name,\n",
    "                dimension=len(embeddings[0]),\n",
    "                metric=\"cosine\",\n",
    "                spec=ServerlessSpec(\n",
    "                    cloud='aws',\n",
    "                    region='us-east-1'\n",
    "                )\n",
    "            )\n",
    "            print(\"Index created successfully\")\n",
    "        \n",
    "        index = pc.Index(index_name)\n",
    "\n",
    "        # Clear existing data if requested (recommended for stock data)\n",
    "        if clear_existing:\n",
    "            print(\"Clearing existing data from index...\")\n",
    "            index.delete(delete_all=True)\n",
    "            print(\"Index cleared successfully\")\n",
    "\n",
    "        vectors = []\n",
    "        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "            vector_id = f\"{chunk.metadata.get('Ticker', 'unknown')}_{i}_{chunk.metadata.get('Update_Date', '')}\"\n",
    "            \n",
    "            metadata = {\n",
    "                'Ticker': chunk.metadata.get('Ticker'),\n",
    "                'Company_Name': chunk.metadata.get('Company_Name'),\n",
    "                'Sector': chunk.metadata.get('Sector'),\n",
    "                'Industry': chunk.metadata.get('Industry'),\n",
    "\n",
    "                'content': chunk.page_content,\n",
    "                \n",
    "                'chunk_index': i\n",
    "            }\n",
    "            \n",
    "            vectors.append({\n",
    "                \"id\": vector_id,\n",
    "                \"values\": embedding,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(vectors), batch_size):\n",
    "            batch = vectors[i:i + batch_size]\n",
    "            index.upsert(vectors=batch)\n",
    "            print(f\"Upserted batch {i//batch_size + 1}/{(len(vectors) + batch_size - 1)//batch_size}\")\n",
    "\n",
    "        print(f\"Successfully saved {len(vectors)} embeddings to Pinecone index '{index_name}'\")\n",
    "        return f\"Saved {len(vectors)} embeddings to Pinecone\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save embeddings to Pinecone: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f98fd8",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1f44e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sandp500 = pd.read_csv('/Users/ani/Projects/6_stock_portfolio_recommendation/data/stock_data.csv')\n",
    "df_sandp500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e87efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = connect_to_pinecone(pinecone_api)\n",
    "embeddings_model = connect_to_huggingface_embeddings(huggingface_embeddings_model)\n",
    "chunks = create_text_chunks(df_sandp500)\n",
    "embeddings = create_embeddings_with_model(chunks, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0039c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_to_pinecone(pc, chunks, embeddings, index_name=pinecone_index_name, clear_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f419d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
